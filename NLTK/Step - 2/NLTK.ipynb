{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc9b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c120a",
   "metadata": {},
   "source": [
    "## Tokenization: Breaking text into words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9416a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I am learning Natural Language Processing!\"\n",
    "word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3fb51",
   "metadata": {},
   "source": [
    "#### Stopwoeds Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a687b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7afe65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word = [w for w in word_tokenize(text) if w.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f387b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning', 'Natural', 'Language', 'Processing', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5e965",
   "metadata": {},
   "source": [
    "## Text cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c8a4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import  stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Setup \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import  stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Setup \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    token = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    token = [word for word in token if word not in stop_words]\n",
    "    \n",
    "    # Lemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in token]\n",
    "    \n",
    "    return words\n",
    "text = \"I am learning Natural Language Processing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab14e7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['striped', 'bat', 'hanging', 'foot', 'best']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "clean_text(\"The striped bats are hanging on their feet for best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35ccd6",
   "metadata": {},
   "source": [
    "#### Bag of words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cbc2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5cac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['I love NLP', \"NLP is fun!\", \"Fun with NLP and Python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cef17c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "428b151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'fun' 'is' 'love' 'nlp' 'python' 'with']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75313ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8ae0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X = tfidf.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c27504e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'fun' 'is' 'love' 'nlp' 'python' 'with']\n",
      "[[0.         0.         0.         0.861037   0.50854232 0.\n",
      "  0.        ]\n",
      " [0.         0.54783215 0.72033345 0.         0.42544054 0.\n",
      "  0.        ]\n",
      " [0.50461134 0.38376993 0.         0.         0.29803159 0.50461134\n",
      "  0.50461134]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18550e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')  # your own token pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68dbba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a9a29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'fun' 'i' 'is' 'love' 'nlp' 'python' 'with']\n",
      "[[0 0 0 1 1 0 0]\n",
      " [0 1 1 0 1 0 0]\n",
      " [1 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41679406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'love' 'nlp' 'python']\n",
      "[[0.         0.861037   0.50854232 0.        ]\n",
      " [0.78980693 0.         0.61335554 0.        ]\n",
      " [0.54783215 0.         0.42544054 0.72033345]]\n",
      "[[0.         0.861037   0.50854232 0.        ]\n",
      " [0.78980693 0.         0.61335554 0.        ]\n",
      " [0.54783215 0.         0.42544054 0.72033345]]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import  stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Setup \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    token = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    token = [word for word in token if word not in stop_words]\n",
    "    \n",
    "    # Lemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in token]\n",
    "    \n",
    "    return words\n",
    "text = \"I am learning Natural Language Processing!\"\n",
    "\n",
    "def vectorizer(docs):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    return vectorizer, X\n",
    "\n",
    "# Original docs\n",
    "docs = ['I love NLP', \"NLP is fun!\", \"Fun with NLP and Python\"]\n",
    "\n",
    "# Clean the text and join tokens back into strings\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = ' '.join(clean_text(docs[i]))\n",
    "\n",
    "vectorizer, X = vectorizer(docs)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2770c0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['love', 'nlp'], ['nlp', 'fun'], ['fun', 'nlp', 'python']]\n"
     ]
    }
   ],
   "source": [
    "docs = ['I love NLP', \"NLP is fun!\", \"Fun with NLP and Python\"]\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = clean_text(docs[i])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccbe88b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = \"I love you \"\n",
    "clean_text(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19eca72",
   "metadata": {},
   "source": [
    "### Top Phrases Extracor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04fa9841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "import nltk \n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d501eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    token = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    token = [word for word in token if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in token]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "983a0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I absolutely love machine learning and artificial intelligence!\",\n",
    "    \"Natural language processing is a fascinating field of AI.\",\n",
    "    \"I hate bugs in my code but I love debugging.\",\n",
    "    \"Deep learning and neural networks are powerful tools.\"\n",
    "]\n",
    "\n",
    "# Clean the documents\n",
    "cleaned_docs = [clean_text(doc) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "177f9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(cleaned_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bf6162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(), columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89b7f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_phrases_per_doc(tfidf, top_n=5):\n",
    "    top_phrases = []\n",
    "    for i, row in tfidf.iterrows():\n",
    "        sorted_row = row.sort_values(ascending=False)\n",
    "        top = sorted_row.head(top_n)\n",
    "        top_phrases.append(list(zip(top.index, top.values)))\n",
    "    return top_phrases\n",
    "top_phrases = get_top_phrases_per_doc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67dd73b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('absolutely', np.float64(0.3124514068609995)),\n",
       "  ('machine learning', np.float64(0.3124514068609995)),\n",
       "  ('absolutely love', np.float64(0.3124514068609995)),\n",
       "  ('intelligence', np.float64(0.3124514068609995)),\n",
       "  ('love machine', np.float64(0.3124514068609995))],\n",
       " [('language processing', np.float64(0.3015113445777636)),\n",
       "  ('fascinating', np.float64(0.3015113445777636)),\n",
       "  ('language', np.float64(0.3015113445777636)),\n",
       "  ('natural', np.float64(0.3015113445777636)),\n",
       "  ('field ai', np.float64(0.3015113445777636))],\n",
       " [('love debugging', np.float64(0.34056989045654285)),\n",
       "  ('bug', np.float64(0.34056989045654285)),\n",
       "  ('bug code', np.float64(0.34056989045654285)),\n",
       "  ('code', np.float64(0.34056989045654285)),\n",
       "  ('code love', np.float64(0.34056989045654285))],\n",
       " [('tool', np.float64(0.3068351989349343)),\n",
       "  ('neural', np.float64(0.3068351989349343)),\n",
       "  ('learning neural', np.float64(0.3068351989349343)),\n",
       "  ('deep learning', np.float64(0.3068351989349343)),\n",
       "  ('network', np.float64(0.3068351989349343))]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b774998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 Top Phrases:\n",
      "  absolutely: 0.3125\n",
      "  machine learning: 0.3125\n",
      "  absolutely love: 0.3125\n",
      "  intelligence: 0.3125\n",
      "  love machine: 0.3125\n",
      "\n",
      "\n",
      "Document 2 Top Phrases:\n",
      "  language processing: 0.3015\n",
      "  fascinating: 0.3015\n",
      "  language: 0.3015\n",
      "  natural: 0.3015\n",
      "  field ai: 0.3015\n",
      "\n",
      "\n",
      "Document 3 Top Phrases:\n",
      "  love debugging: 0.3406\n",
      "  bug: 0.3406\n",
      "  bug code: 0.3406\n",
      "  code: 0.3406\n",
      "  code love: 0.3406\n",
      "\n",
      "\n",
      "Document 4 Top Phrases:\n",
      "  tool: 0.3068\n",
      "  neural: 0.3068\n",
      "  learning neural: 0.3068\n",
      "  deep learning: 0.3068\n",
      "  network: 0.3068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, phrases in enumerate(top_phrases):\n",
    "    print(f\"\\nDocument {i+1} Top Phrases:\")\n",
    "    for phrase, score in phrases:\n",
    "        print(f\"  {phrase}: {score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d094a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
