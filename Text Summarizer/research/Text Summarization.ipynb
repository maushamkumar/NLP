{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331aba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaymahto/anaconda3/envs/textsum38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import  Pipeline, set_seed\n",
    "from datasets import  load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datasets import load_dataset, load as load_metric\n",
    "from transformers import  AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f9d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")      # Apple-GPU (M1/M2/M3)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")     # NVIDIA GPU on other machines\n",
    "else:\n",
    "    device = torch.device(\"cpu\")      # fallback\n",
    "\n",
    "print(\"Running on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2964ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = 'google/pegasus-cnn_dailymail'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model_peasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "# pegasus = Pipeline(\n",
    "#     \"summarization\",\n",
    "#     model=model_peasus,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0 if torch.cuda.is_available() else -1,\n",
    "#     max_length=512,\n",
    "#     min_length=30,\n",
    "#     length_penalty=2.0,\n",
    "#     num_beams=4,\n",
    "#     early_stopping=True\n",
    "# )\n",
    "# def summarize_text(text):\n",
    "#     \"\"\"\n",
    "#     Summarizes the input text using the Pegasus model.\n",
    "#     \"\"\"\n",
    "#     # Tokenize the text into sentences\n",
    "#     sentences = sent_tokenize(text)\n",
    "    \n",
    "#     # Join the sentences back into a single string\n",
    "#     text = ' '.join(sentences)\n",
    "    \n",
    "#     # Generate the summary\n",
    "#     summary = pegasus(text, max_length=130, min_length=30, do_sample=False)\n",
    "    \n",
    "#     return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c780475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: brew\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# ‚ù∂ Install wget once if it isn't present\n",
    "!brew install wget          # or use the curl/Python alternatives above\n",
    "\n",
    "# ‚ù∑ Download\n",
    "!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
    "\n",
    "# ‚ù∏ Unzip (optional)\n",
    "# !unzip -o summarizer-data.zip -d data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7337cdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum = load_from_disk(\"samsum_dataset\")\n",
    "dataset_samsum\n",
    "# /Users/sanjaymahto/NLP-1/Text Summarizer/research/data/samsum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc3c8311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features : ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogues\n",
      "Wanda: Let's make a party!\n",
      "Gina: Why?\n",
      "Wanda: beacuse. I want some fun!\n",
      "Gina: ok, what do u need?\n",
      "Wanda: 1st I need too make a list\n",
      "Gina: noted and then?\n",
      "Wanda: well, could u take yours father car and go do groceries with me?\n",
      "Gina: don't know if he'll agree\n",
      "Wanda: I know, but u can ask :)\n",
      "Gina: I'll try but theres no promisess\n",
      "Wanda: I know, u r the best!\n",
      "Gina: When u wanna go\n",
      "Wanda: Friday?\n",
      "Gina: ok, I'll ask\n",
      "\n",
      "Summary\n",
      "Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries together. They set the date for Friday. \n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum.keys()]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features : {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogues\")\n",
    "\n",
    "print(dataset_samsum['test'][10]['dialogue'])\n",
    "print(\"\\nSummary\")\n",
    "\n",
    "print(dataset_samsum['test'][10]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d989b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True, padding='max_length')\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True, padding='max_length')\n",
    "        \n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2427d68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]/Users/sanjaymahto/anaconda3/envs/textsum38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14732/14732 [00:25<00:00, 570.22 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 819/819 [00:01<00:00, 597.58 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 818/818 [00:01<00:00, 812.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_example_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13dafc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d029d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model \n",
    "from transformers import  DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_peasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6092370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaymahto/anaconda3/envs/textsum38/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import  TrainingArguments, Trainer \n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"pegasus-samsum\",\n",
    "    num_train_epochs=1, \n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1, \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1e6, \n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c4012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/8rqwg0nd7995t2tzgq40dsph0000gn/T/ipykernel_36369/274080829.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_peasus, args = trainer_args, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=seq2seq_data_collator,\n",
    "    train_dataset=dataset_samsum_pt['test'],\n",
    "    eval_datase t=dataset_samsum_pt['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa48bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe37954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, batch_size=16, device=device,\n",
    "                                column_text=\"article\", column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "\n",
    "    for article_batch, target in tqdm(\n",
    "      zip(article_batches, target_batches), total=len(article_batches)\n",
    "\n",
    "  ):\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs['input_ids'].to(device),\n",
    "                               attention_mask=input['attention_mask'].to(device),\n",
    "                               length_penalty=0.8,\n",
    "                               num_beams=8,\n",
    "                               max_length=128)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target)\n",
    "    \n",
    "    # Finally compute and return the ROUGE Scores, \n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef67b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5168c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    dataset_samsum_pt['test'][1:10],\n",
    "    rouge_metric, \n",
    "    trainer.model, \n",
    "    tokenizer,\n",
    "    batch_size=2, \n",
    "    device=device,\n",
    "    column_text=\"dialogue\",\n",
    "    column_summary=\"summary\" \n",
    "    \n",
    ")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\n",
    "pd.DataFrame(rouge_dict, index = [f'pegasus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model \n",
    "model_peagus.save_pretrained(\"pegasus-samsum.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a603bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save tokenizer \n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e18130",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load \n",
    "tokenizer = AutoTokenizer.from_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction \n",
    "gen_kwargs = {\n",
    "    \"length_penalty\": 0.8,\n",
    "    \"num_beams\": 8,\n",
    "    \"max_length\": 128,\n",
    "}\n",
    "\n",
    "sample_text = dataset_samsum['test'][0]['dialogue']\n",
    "\n",
    "reference = dataset_samsum['test'][0]['summary']\n",
    "\n",
    "pipe = Pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"model_peagus\",\n",
    "    tokenizer=tokenizer,\n",
    "    # device=0 if torch.cuda.is_available() else -1,\n",
    "    # max_length=128,\n",
    "    # min_length=30,\n",
    "    # length_penalty=2.0,\n",
    "    # num_beams=4,\n",
    "    # early_stopping=True\n",
    ")\n",
    "\n",
    "## \n",
    "print(\"Dialogue\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"\\nReference Summary: \")\n",
    "print(reference)\n",
    "\n",
    "print(\"\\nModel Summary: \")\n",
    "print(pipe(sample_text, **gen_kwargs)[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "0eb9ebeaf3e16e04e89c4068999b1193eb54255b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d35eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution('abc', 'bc') # returns true\n",
    "solution('abc', 'd') # returns false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f971482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution('abc', 'bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4f266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 'abc'\n",
    "end = 'bc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca80d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "bc\n"
     ]
    }
   ],
   "source": [
    "n = len(end)\n",
    "print(n)\n",
    "start = start[-n:]\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1278a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(starting, ending):\n",
    "    n = len(ending)\n",
    "    starting = starting[-n:]\n",
    "    if starting == ending: \n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ce44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsum38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
